{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**문장 토큰화**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7475f7880ef504d3"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Select your preferences and run the install command.', 'Stable represents the most currently tested and supported version of PyTorch.', 'This should be suitable for many users.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "text_sample = \"Select your preferences and run the install command. Stable represents the most currently tested and supported version of PyTorch. This should be suitable for many users.\"\n",
    "tokenized_sentences = sent_tokenize(text_sample)\n",
    "print(tokenized_sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T06:23:39.221879100Z",
     "start_time": "2023-09-25T06:23:39.202391100Z"
    }
   },
   "id": "7c20df5a5de7e91"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**단어 토큰화**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd394fa1d2ab4ad2"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'book', 'is', 'for', 'deep', 'learning', 'learners']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "sentence = \"This book is for deep learning learners\"\n",
    "words = word_tokenize(sentence)\n",
    "print(words)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T06:23:39.225890700Z",
     "start_time": "2023-09-25T06:23:39.205896100Z"
    }
   },
   "id": "5122bd15419ce444"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**아포스트로피가 포함된 문장에서 단어 토큰화**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1bba340228313aa"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', \"'\", 's', 'nothing', 'that', 'you', 'don', \"'\", 't', 'already', 'know', 'except', 'most', 'people', 'aren', \"'\", 't', 'aware', 'of', 'how', 'their', 'inner', 'world', 'works', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "sentence = \"it's nothing that you don't already know except most people aren't aware of how their inner world works.\"\n",
    "words = WordPunctTokenizer().tokenize(sentence)\n",
    "print(words)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T06:23:39.239340200Z",
     "start_time": "2023-09-25T06:23:39.220491700Z"
    }
   },
   "id": "35717f9687b12cc1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**라이브러리 호출 및 데이터셋 준비**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5cc93a2ba8afc94"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import csv\n",
    "from konlpy.tag import Okt\n",
    "from gensim.models import word2vec\n",
    "\n",
    "f = open(r\"../pytorch/data/ratings_train.txt\", \"r\", encoding=\"utf-8\")\n",
    "rdr = csv.reader(f, delimiter=\"\\t\")\n",
    "rdw = list(rdr)\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T06:23:39.476418700Z",
     "start_time": "2023-09-25T06:23:39.224889400Z"
    }
   },
   "id": "3f7e26e592290747"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**오픈 소스 한글 형태소 분석기 호출**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90defe244a61b90f"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "ename": "JVMNotFoundException",
     "evalue": "No JVM shared library file (jvm.dll) found. Try setting up the JAVA_HOME environment variable properly.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mJVMNotFoundException\u001B[0m                      Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m twitter \u001B[38;5;241m=\u001B[39m Okt()\n\u001B[0;32m      3\u001B[0m result \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m rdw:\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\konlpy\\tag\\_okt.py:51\u001B[0m, in \u001B[0;36mOkt.__init__\u001B[1;34m(self, jvmpath, max_heap_size)\u001B[0m\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, jvmpath\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, max_heap_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1024\u001B[39m):\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m jpype\u001B[38;5;241m.\u001B[39misJVMStarted():\n\u001B[1;32m---> 51\u001B[0m         jvm\u001B[38;5;241m.\u001B[39minit_jvm(jvmpath, max_heap_size)\n\u001B[0;32m     53\u001B[0m     oktJavaPackage \u001B[38;5;241m=\u001B[39m jpype\u001B[38;5;241m.\u001B[39mJPackage(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mkr.lucypark.okt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     54\u001B[0m     OktInterfaceJavaClass \u001B[38;5;241m=\u001B[39m oktJavaPackage\u001B[38;5;241m.\u001B[39mOktInterface\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\konlpy\\jvm.py:55\u001B[0m, in \u001B[0;36minit_jvm\u001B[1;34m(jvmpath, max_heap_size)\u001B[0m\n\u001B[0;32m     52\u001B[0m args \u001B[38;5;241m=\u001B[39m [javadir, os\u001B[38;5;241m.\u001B[39msep]\n\u001B[0;32m     53\u001B[0m classpath \u001B[38;5;241m=\u001B[39m [f\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;241m*\u001B[39margs) \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m folder_suffix]\n\u001B[1;32m---> 55\u001B[0m jvmpath \u001B[38;5;241m=\u001B[39m jvmpath \u001B[38;5;129;01mor\u001B[39;00m jpype\u001B[38;5;241m.\u001B[39mgetDefaultJVMPath()\n\u001B[0;32m     57\u001B[0m \u001B[38;5;66;03m# NOTE: Temporary patch for Issue #76. Erase when possible.\u001B[39;00m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m sys\u001B[38;5;241m.\u001B[39mplatform \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdarwin\u001B[39m\u001B[38;5;124m'\u001B[39m\\\n\u001B[0;32m     59\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m jvmpath\u001B[38;5;241m.\u001B[39mfind(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m1.8.0\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\\\n\u001B[0;32m     60\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m jvmpath\u001B[38;5;241m.\u001B[39mendswith(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlibjvm.dylib\u001B[39m\u001B[38;5;124m'\u001B[39m):\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\jpype\\_jvmfinder.py:74\u001B[0m, in \u001B[0;36mgetDefaultJVMPath\u001B[1;34m()\u001B[0m\n\u001B[0;32m     72\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     73\u001B[0m     finder \u001B[38;5;241m=\u001B[39m LinuxJVMFinder()\n\u001B[1;32m---> 74\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m finder\u001B[38;5;241m.\u001B[39mget_jvm_path()\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\jpype\\_jvmfinder.py:212\u001B[0m, in \u001B[0;36mJVMFinder.get_jvm_path\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    210\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m jvm_notsupport_ext \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    211\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m jvm_notsupport_ext\n\u001B[1;32m--> 212\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m JVMNotFoundException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo JVM shared library file (\u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m) \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    213\u001B[0m                            \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfound. Try setting up the JAVA_HOME \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    214\u001B[0m                            \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menvironment variable properly.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    215\u001B[0m                            \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_libfile))\n",
      "\u001B[1;31mJVMNotFoundException\u001B[0m: No JVM shared library file (jvm.dll) found. Try setting up the JAVA_HOME environment variable properly."
     ]
    }
   ],
   "source": [
    "twitter = Okt()\n",
    "\n",
    "result = []\n",
    "\n",
    "for line in rdw:\n",
    "    malist = twitter.pos(line[1], norm=True, stem=True)\n",
    "    r = []\n",
    "    for word in malist:\n",
    "        if not word[1] in [\"Josa\", \"Eomi\", \"Punctuation\"]:\n",
    "            r.append(word[0])\n",
    "    rl = (\" \".join(r)).strip()\n",
    "    result.append(rl)\n",
    "    print(rl)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T06:23:39.510752800Z",
     "start_time": "2023-09-25T06:23:39.478757700Z"
    }
   },
   "id": "82a3ff5b41d4f00a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**형태소 저장**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22469f955578296b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"NaverMovie.nlp\", \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(\"\\n\".join(result))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "868b9e6ec3116940"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Word2Vec 모델 생성**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65ea98fbd8bf36ed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mData = word2vec.LineSentence(\"NaverMovie.nlp\")\n",
    "mModel = word2vec.Word2Vec(mData, vector_size=200, window=10, hs=1, min_count=2, sg=1)\n",
    "mModel.save(\"NaverMovie.model\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f37cd21782f4c907"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**불용어 제거**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eadc828d6ec5c2ab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sample_text = \"Select your preferences and run the install command. Stable represents the most currently tested and supported version of PyTorch. This should be suitable for many users.\"\n",
    "text_tokens = word_tokenize(sample_text)\n",
    "\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stopwords.words(\"english\")]\n",
    "print(\"불용어 제거 미적용: \", text_tokens, \"\\n\")\n",
    "print(\"불용어 제거 적용: \", tokens_without_sw)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46f1cd1d7f4b094f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**포터 알고리즘**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71bbe78961bb79df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "print(stemmer.stem(\"obesses\"), stemmer.stem(\"obssesed\"))\n",
    "print(stemmer.stem(\"standardizes\"), stemmer.stem(\"standardization\"))\n",
    "print(stemmer.stem(\"national\"), stemmer.stem(\"nation\"))\n",
    "print(stemmer.stem(\"absentness\"), stemmer.stem(\"absently\"))\n",
    "print(stemmer.stem(\"tribalical\"), stemmer.stem(\"tribalicalized\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81c6151550698ed7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**랭커스터 알고리즘**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4f8bec08acd22c4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(stemmer.stem(\"obesses\"), stemmer.stem(\"obssesed\"))\n",
    "print(stemmer.stem(\"standardizes\"), stemmer.stem(\"standardization\"))\n",
    "print(stemmer.stem(\"national\"), stemmer.stem(\"nation\"))\n",
    "print(stemmer.stem(\"absentness\"), stemmer.stem(\"absently\"))\n",
    "print(stemmer.stem(\"tribalical\"), stemmer.stem(\"tribalicalized\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f90a5cbb60ad42e0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**표제어 추출**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d3ef8d4f69e2156"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "print(stemmer.stem(\"obesses\"), stemmer.stem(\"obssesed\"))\n",
    "print(lemma.lemmatize(\"standardizes\"), lemma.lemmatize(\"standardization\"))\n",
    "print(lemma.lemmatize(\"national\"), lemma.lemmatize(\"nation\"))\n",
    "print(lemma.lemmatize(\"absentness\"), lemma.lemmatize(\"absently\"))\n",
    "print(lemma.lemmatize(\"tribalical\"), lemma.lemmatize(\"tribalicalized\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65d9ffa32f173c2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**품사 정보가 추가된 표제어 추출**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "acf656976eb5f2a0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(lemma.lemmatize(\"obesses\", \"v\"), lemma.lemmatize(\"obssesed\", \"a\"))\n",
    "print(lemma.lemmatize(\"standardizes\", \"v\"), lemma.lemmatize(\"standardization\", \"n\"))\n",
    "print(lemma.lemmatize(\"national\", \"a\"), lemma.lemmatize(\"nation\", \"n\"))\n",
    "print(lemma.lemmatize(\"absentness\", \"n\"), lemma.lemmatize(\"absently\", \"r\"))\n",
    "print(lemma.lemmatize(\"tribalical\", \"a\"), lemma.lemmatize(\"tribalicalized\", \"v\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44b332afd807399d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
